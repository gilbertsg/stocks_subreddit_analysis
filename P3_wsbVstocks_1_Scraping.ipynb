{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6b4d5d-f447-4c84-bc67-4a35f8790bd5",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc82fec-00dd-4ad5-8753-cc4cb05687f6",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "Our clients (Robinhood Markets Inc.) aims to expand its services from serving short-term options/stocks traders (such as those found in r/WallStreetBets) and start serving long-term investors (such as those found in r/stocks).  However, since these two subreddits have different interests, jargons, and audience, they would need to properly target the advertisement to the correct subreddit (r/WallStreetBets would not be interested in long-term investment). Hence, we are tasked with developing a model that can classify whether a post belongs to to subreddit r/WallStreetBets or r/stocks, In order to serve the correct post with the corresponding advertisement.\n",
    "\n",
    "## Introduction:\n",
    "r/WallStreetBets (also known as r/wsb) is a subreddit for discussing stocks and option trading. It has become notable for its colorful and profane jargon, aggressive trading strategies, harassment, and for playing a major role in the GameStop short squeeze that caused losses for some US firms and short sellers in a few days in early 2021 [[1]](https://en.wikipedia.org/wiki/R/wallstreetbets). The posts in r/wsb is dominated with memes, propsal/ideas for extremely risky stock/option plays, as well as reports about massive gains/losses from said plays. \n",
    "\n",
    "On the other hand r/stocks is a subreddit for for a more serious discussion on stocks and options, where the participants usually posts analysis and discussions on various stocks and companies. Discussions on highly risky plays on stocks with low capitalization and volume (typically known as \"Penny Stocks\") are outright banned in the subreddit. Instead, the type of discussions conducted at stocks is more geared towards serious long-term investments, which is spcifically the target of Robinhood's new expansion plan.\n",
    "\n",
    "## Scope:\n",
    "For this project, will be scraping all the posts from both subreddits in the period between August 2021 to August 2022. The reason for selecting this particular time period is because several months prior to August 2021, the majority of the discussions in both subreddits are still revolving around the GameStop short squeeze [[2]](https://en.wikipedia.org/wiki/GameStop_short_squeeze). The jargon, vocabulary, and talking points of this particular topic is quite different compared to subsequent topics. As such we have decided to not include the discussion of this particular topic on the analysis and classification project.\n",
    "\n",
    "## Success Evaluation:\n",
    "The difficulty for this task comes from the fact that this is a highly imbalanced classification problem. There are nearly 5x more posts coming from r/WSB then they are from r/stocks. As such using the simple accuracy metric (i.e.: ratio of correct predictions) would result in an erroneously high performance metric. For this project we'll be focusing on the precision and recall [[3]](https://en.wikipedia.org/wiki/Precision_and_recall) in predicting the target class. The following are the metric definition in the context of advertising to the target class:\n",
    "- True positive: correctly classifying and serving the advertisement to the target class (r/stocks)\n",
    "- False positive: incorrectly classifying the target class (r/stocks), and instead serving the advertisment to the wrong subreddit (r/wsb)\n",
    "- True negative: correctly clasifying the other class (r/wsb) and not serving the advertisement\n",
    "- False negative: incorrectly classifying the other class (r/wsb) which resulted in not serving the advertisement to the target class (r/stocks)\n",
    "- Precision: ratio of advertisement served to the correct class\n",
    "- Recall: ratio of posts in the correct class that is correctly served the advertisement\n",
    "\n",
    "Based on the definitions above we are aiming to strike a balance between precision and recall, where the client is able to have a wide enough coverage in serving the advertisement to the target class (recall) while still maintaining a good enough precision so as not to waste the advertising budget on the wrong class. As such, we can use the f1-score [[4]](https://en.wikipedia.org/wiki/F-score) which takes into account both of the previous metrics in consideration.\n",
    "\n",
    "## Secondary Objective:\n",
    "The secondary objective for this project is to analyze the correlation between the subreddits' sentiments on a particular stock against the future performance of that stock (defined as price change in 7-days). This is to assess whether these subreddits have any predictive capability for making stock picks. If we find that these subreddits are able to have some predictive capability, we can use the subreddits prediction to inform/supplement the analysis of the clients' Investment team in making their stock purchase decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0cb2a-e320-4111-868b-451f19ba95dd",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50621a25-1baf-497e-a806-0f76bca85a15",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Note: To access the pre-scraped datasets, you will need to export the the .rar files from the 'data_compressed' folder, and put them in the 'data' folder</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e4e99-b369-4262-a7f8-837b0beabaed",
   "metadata": {},
   "source": [
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48cc389-44ec-4f4b-8bb2-b1b69d104522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## library imports\n",
    "\n",
    "# data processing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scraping imports\n",
    "import requests\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "# misc imports\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65e9bf-6e04-4a7d-bb65-cd38d80e0a17",
   "metadata": {},
   "source": [
    "## Subreddit posts scraping using PMAW and Pushshift.io"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cadc97d-ba1f-4a27-97f8-ed8ab2db0c0b",
   "metadata": {},
   "source": [
    "WARNING: THE CODE BLOCK BELOW WILL TAKE ABOUT 40 MINUTES TO RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36543204-529b-4247-ae18-b785bbd4dde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a | Obtained 25367 posts from 'r/wallstreetbets' between 08/2021 and 09/2021 | elapsed time: 3.25 mins\n",
      "1b | Obtained 4567 posts from 'r/stocks' between 08/2021 and 09/2021 | elapsed time: 0.69 mins\n",
      "2a | Obtained 25574 posts from 'r/wallstreetbets' between 09/2021 and 10/2021 | elapsed time: 2.83 mins\n",
      "2b | Obtained 4176 posts from 'r/stocks' between 09/2021 and 10/2021 | elapsed time: 0.48 mins\n",
      "3a | Obtained 25911 posts from 'r/wallstreetbets' between 10/2021 and 11/2021 | elapsed time: 3.09 mins\n",
      "3b | Obtained 4441 posts from 'r/stocks' between 10/2021 and 11/2021 | elapsed time: 0.52 mins\n",
      "4a | Obtained 28085 posts from 'r/wallstreetbets' between 11/2021 and 12/2021 | elapsed time: 3.21 mins\n",
      "4b | Obtained 5457 posts from 'r/stocks' between 11/2021 and 12/2021 | elapsed time: 0.63 mins\n",
      "5a | Obtained 20304 posts from 'r/wallstreetbets' between 12/2021 and 01/2022 | elapsed time: 2.64 mins\n",
      "5b | Obtained 4340 posts from 'r/stocks' between 12/2021 and 01/2022 | elapsed time: 0.54 mins\n",
      "6a | Obtained 22632 posts from 'r/wallstreetbets' between 01/2022 and 02/2022 | elapsed time: 2.73 mins\n",
      "6b | Obtained 5086 posts from 'r/stocks' between 01/2022 and 02/2022 | elapsed time: 0.68 mins\n",
      "7a | Obtained 14723 posts from 'r/wallstreetbets' between 02/2022 and 03/2022 | elapsed time: 1.62 mins\n",
      "7b | Obtained 3958 posts from 'r/stocks' between 02/2022 and 03/2022 | elapsed time: 0.46 mins\n",
      "8a | Obtained 16215 posts from 'r/wallstreetbets' between 03/2022 and 04/2022 | elapsed time: 1.86 mins\n",
      "8b | Obtained 3935 posts from 'r/stocks' between 03/2022 and 04/2022 | elapsed time: 0.49 mins\n",
      "9a | Obtained 16398 posts from 'r/wallstreetbets' between 04/2022 and 05/2022 | elapsed time: 1.85 mins\n",
      "9b | Obtained 3714 posts from 'r/stocks' between 04/2022 and 05/2022 | elapsed time: 0.47 mins\n",
      "10a | Obtained 17582 posts from 'r/wallstreetbets' between 05/2022 and 06/2022 | elapsed time: 2.42 mins\n",
      "10b | Obtained 3806 posts from 'r/stocks' between 05/2022 and 06/2022 | elapsed time: 0.43 mins\n",
      "11a | Obtained 14202 posts from 'r/wallstreetbets' between 06/2022 and 07/2022 | elapsed time: 1.64 mins\n",
      "11b | Obtained 2779 posts from 'r/stocks' between 06/2022 and 07/2022 | elapsed time: 0.47 mins\n",
      "12a | Obtained 13771 posts from 'r/wallstreetbets' between 07/2022 and 08/2022 | elapsed time: 1.92 mins\n",
      "12b | Obtained 2796 posts from 'r/stocks' between 07/2022 and 08/2022 | elapsed time: 0.3 mins\n",
      "SCRAPING COPMLETED! Obtained a total of 289819 posts | Total elapsed time: 35.22 mins\n"
     ]
    }
   ],
   "source": [
    "scrape_start = datetime.datetime(2021, 8, 1) # set the start date for scraping\n",
    "scrape_months = 12 # indicate the total duration of subreddits to be scraped (in months)\n",
    "\n",
    "total_scraped_post = 0 # initialize total scraped post counter\n",
    "total_timer = 0 # initialize total timer\n",
    "\n",
    "wsb = pd.DataFrame() # instantiate empty dataframe to contain scraped wsb posts\n",
    "stocks = pd.DataFrame() # instantiate empty dataframe to contain scraped stocks posts\n",
    "\n",
    "for i in range(scrape_months): # looping through the number of months to be scraped\n",
    "    \n",
    "    # the subreddits' post data will be obtained through the PMAW, which is a wrapper of the PushShift API\n",
    "    # the subreddits will be scraped in a monthly interval, this will be done by specifying the timestamp 'after' and 'before' which the subreddits will be scraped\n",
    "    after = scrape_start+relativedelta(months=+(i)) # setting the 'after' datetime as a function of the start date and the current month increment in the loop\n",
    "    before = scrape_start+relativedelta(months=+(i+1)) # setting the 'before' datetime as a function of the start date and the current month increment in the loop\n",
    "    \n",
    "    # converting the datetime to a timestamp\n",
    "    scrape_after = int(after.timestamp())\n",
    "    scrape_before = int(before.timestamp())\n",
    "    \n",
    "    \n",
    "    # SCRAPING THE WALLSTREETBETS SUBREDDIT\n",
    "    \n",
    "    start_timer = time.time() # starting the timer (for displayinig the time required to scrape one particular month of the subreddit)\n",
    "    \n",
    "    wsb_posts = PushshiftAPI().search_submissions(subreddit=\"wallstreetbets\", # using the PMAW wrapper for pushhift API to scrape the wsb subreddit\n",
    "                                              limit=31*1000, # obtain a maximum of 310000 posts (or all the posts available in the month), assuming at most 1000 posts per day\n",
    "                                              after=scrape_after, before=scrape_before) # setting the time boundaries of the scraping\n",
    "    \n",
    "    wsb_current_month = [post for post in wsb_posts] # storing the month's post in a list of dictionaries\n",
    "    wsb_current_month = pd.DataFrame(wsb_current_month) # converting the list to a dataframe\n",
    "\n",
    "    wsb = pd.concat([wsb,wsb_current_month]) # concatenating the current month's dataframe with the total dataframe\n",
    "    \n",
    "    duration_timer = round((time.time() - start_timer)/60, 2) # stopping the timer and storing the elapsed time in minutes\n",
    "    total_timer += duration_timer # incrementing the total timer with the current timer\n",
    "    total_scraped_post += wsb_current_month.shape[0] # incrementing the total post count with the current post count\n",
    "    \n",
    "    # displaying the report for the current month\n",
    "    print(f\"{i+1}a | Obtained {wsb_current_month.shape[0]} posts from 'r/wallstreetbets' between {after.strftime('%m/%Y')} and {before.strftime('%m/%Y')} | elapsed time: {duration_timer} mins\")\n",
    "    \n",
    "    \n",
    "    # SCRAPING THE STOCKS SUBREDDIT\n",
    "    # starting the timer (for displayinig the time required to scrape one particular month of the subreddit)\n",
    "    start_timer = time.time()\n",
    "    \n",
    "    stocks_posts = PushshiftAPI().search_submissions(subreddit=\"stocks\", # using the PMAW wrapper for pushhift API to scrape the stocks subreddit\n",
    "                                              limit=31*400, # below 400 posts per day\n",
    "                                              after=scrape_after, before=scrape_before) # setting the time boundaries of the scraping\n",
    "        \n",
    "    stocks_current_month = [post for post in stocks_posts] # storing the month's post in a list of dictionaries\n",
    "    stocks_current_month = pd.DataFrame(stocks_current_month) # converting the list to a dataframe\n",
    "\n",
    "    stocks = pd.concat([stocks,stocks_current_month]) # concatenating the current month's dataframe with the total dataframe\n",
    "    \n",
    "    duration_timer = round((time.time() - start_timer)/60, 2) # stopping the timer and storing the elapsed time in minutes\n",
    "    total_timer += duration_timer # incrementing the total timer with the current timer\n",
    "    total_scraped_post += stocks_current_month.shape[0] # incrementing the total post count with the current post count\n",
    "    \n",
    "    # displaying the report for the current month\n",
    "    print(f\"{i+1}b | Obtained {stocks_current_month.shape[0]} posts from 'r/stocks' between {after.strftime('%m/%Y')} and {before.strftime('%m/%Y')} | elapsed time: {duration_timer} mins\")\n",
    "\n",
    "# displaying the overall report for the whole scraping process\n",
    "print(f\"SCRAPING COPMLETED! Obtained a total of {total_scraped_post} posts | Total elapsed time: {total_timer} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797eb819-86fd-41a0-9481-e785e1fbad8f",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20896789-edd6-468a-8e46-895afd654066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the dataframe to a csv file\n",
    "wsb.to_csv('data/wsb.csv') \n",
    "stocks.to_csv('data/stocks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
